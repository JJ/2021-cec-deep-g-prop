

@Misc{mazzawi21,
  author =    {Mazzawi, Hanna and Gonzalvo, Xavi},
  title =     {Introducing Model Search: An Open Source Platform for Finding Optimal ML Models},
  howpublished = {Google AI blog, \url{https://ai.googleblog.com/2021/02/introducing-model-search-open-source.html}},
  month =     {February},
  year =      2021}

@article{SENHAJI20201,
title = {Training feedforward neural network via multiobjective optimization model using non-smooth L1/2 regularization},
journal = {Neurocomputing},
volume = {410},
pages = {1-11},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220309115},
author = {Kaoutar Senhaji and Hassan Ramchoun and Mohamed Ettaouil},
keywords = {Multiobjective optimization, , Learning algorithm,  regularization, Neural network},
abstract = {The paper presents a new approach to optimize the Multilayer Perceptron Neural Network (MLPNN), to deal with the generalization problem. As known, most supervised learning algorithms aim to minimize the training error. However, the mentioned methods, based only on error minimizing, may generate a solution with an insufficient generalization performance. This present work proposes a multiobjective modelling problem involving two objectives: accuracy and complexity since the learning problem is multiobjective by nature. The learning task is carried on by minimizing both objectives simultaneously, according to Pareto domination concept, using NSGAII (Non-dominated Sorting Genetic Algorithm II) as a solver. This method leads us to a set of solutions called Pareto front, being the optimal solutions set, the adequate MLPNN need to be extracted. We show empirically that the proposed method is capable of reducing the neural networks topology and improved generalization performance, in addition to a good classification rate compared to different methods.}
}

@article{Martinez2021LightsAS,
  title={Lights and shadows in Evolutionary Deep Learning: Taxonomy, critical methodological analysis, cases of study, learned lessons, recommendations and challenges},
  author={Aritz D. Martinez and J. Ser and Esther Villar-Rodriguez and E. Osaba and Javier Poyatos and S. Tabik and D. Molina and F. Herrera},
  journal={Inf. Fusion},
  year={2021},
  volume={67},
  pages={161-194}
}

@article{GOMESPEREIRADELACERDA2021100777,
title = "A systematic literature review on general parameter control for evolutionary and swarm-based algorithms",
journal = "Swarm and Evolutionary Computation",
volume = "60",
pages = "100777",
year = "2021",
issn = "2210-6502",
doi = "https://doi.org/10.1016/j.swevo.2020.100777",
url = "http://www.sciencedirect.com/science/article/pii/S2210650220304302",
author = "Marcelo {Gomes Pereira de Lacerda} and Luis Filipe {de Araujo Pessoa} and Fernando {Buarque de Lima Neto} and Teresa Bernarda Ludermir and Herbert Kuchen",
keywords = "Parameter control, Evolutionary algorithms, Swarm intelligence, Systematic literature review",
abstract = "This paper presents a systematic literature review on general parameter control for evolutionary and swarm-based algorithms. General methods can be applied to any algorithm, parameter or problem, in contrast to methods that are tailored to specific applications. In this literature review, a total of 4449 studies were retrieved by the search engines and only 50 of them were selected to the extraction phase. Finally, only 15 were fully analyzed and discussed. To the best of our knowledge, this is the first literature review on such a field and one of the very few systematic reviews on parameter adjustment for those algorithms."
}

@INPROCEEDINGS{5360251,

  author={S. {Köknar-Tezel} and L. J. {Latecki}},

  booktitle={2009 Ninth IEEE International Conference on Data Mining},

  title={Improving SVM Classification on Imbalanced Data Sets in Distance Spaces}, 

  year={2009},

  volume={},

  number={},

  pages={259-267},

  doi={10.1109/ICDM.2009.59}}


@article{faris2019automatic,
  title={Automatic selection of hidden neurons and weights in neural networks using grey wolf optimizer based on a hybrid encoding scheme},
  author={Faris, Hossam and Mirjalili, Seyedali and Aljarah, Ibrahim},
  journal={International Journal of Machine Learning and Cybernetics},
  volume={10},
  number={10},
  pages={2901--2920},
  year={2019},
  publisher={Springer},
  notes="Changes size as well as weights"
}

@article{DEVARRIYA2020112866,
title = "Unbalanced breast cancer data classification using novel fitness functions in genetic programming",
journal = "Expert Systems with Applications",
volume = "140",
pages = "112866",
year = "2020",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2019.112866",
url = "http://www.sciencedirect.com/science/article/pii/S0957417419305767",
author = "Divyaansh Devarriya and Cairo Gulati and Vidhi Mansharamani and Aditi Sakalle and Arpit Bhardwaj",
keywords = "Breast cancer, Unbalanced data, Genetic programming, Fitness function",
abstract = "Breast Cancer is a common disease and to prevent it, the disease must be identified at earlier stages. Available breast cancer datasets are unbalanced in nature, i.e. there are more instances of benign (non-cancerous) cases then malignant (cancerous) ones. Therefore, it is a challenging task for most machine learning (ML) models to classify between benign and malignant cases properly, even though they have high accuracy. Accuracy is not a good metric to assess the results of ML models on breast cancer dataset because of biased results. To address this issue, we use Genetic Programming (GP) and propose two fitness functions. First one is F2 score which focuses on learning more about the minority class, which contains more relevant information, the second one is a novel fitness function known as Distance score (D score) which learns about both the classes by giving them equal importance and being unbiased. The GP framework in which we implemented D score is named as D-score GP (DGP) and the framework implemented with F2 score is named as F2GP. The proposed F2GP achieved a maximum accuracy of 99.63\%, 99.51\% and 100\% for 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively and DGP achieves a maximum accuracy of 99.63\%, 98.5\% and 100\% in 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively. The proposed models also achieves a recall of 100\% for all the test cases. This shows that using a new fitness function for unbalanced data classification improves the performance of a classifier."
}

@Inbook{Gupta2020,
author="Gupta, Neeraj
and Khosravy, Mahdi
and Patel, Nilesh
and Gupta, Saurabh
and Varshney, Gazal",
editor="Khosravy, Mahdi
and Gupta, Neeraj
and Patel, Nilesh
and Senjyu, Tomonobu",
title="Artificial Neural Network Trained by Plant Genetic-Inspired Optimizer",
bookTitle="Frontier Applications of Nature Inspired Computation",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="266--280",
abstract="As a great computational intelligence technique, artificial neural networks (ANNs) have intensively attracted the interest of researchers of artificial intelligence. Due to the easy implementation of ANN, vast types of structures and associated rules, their successful application can be seen in real-life and industrial problems. From a wide variety of ANN such as feed-forward ANN, Kohonen self-organizing ANN, radial basis function (RBF) ANN, and spiking ANN, we describe a multi-layer perceptron ANN with the focus on designing AI-based condition monitoring system. This chapter presents the neuroevolution-based monitoring system to detect the oil filter condition in agricultural (Ag) machines using meta-heuristic METO algorithm. Evolutionary learning algorithm finds the optimal weights of ANN along with the behavior of each neuron.",
isbn="978-981-15-2133-1",
doi="10.1007/978-981-15-2133-1_12",
url="https://doi.org/10.1007/978-981-15-2133-1_12",
note="Optimizes weights and training functions"
}

@InProceedings{10.1007/978-981-15-7130-5_38,
author="Singh, Parul
and Ranga, Virender",
editor="Marriwala, Nikhil
and Tripathi, C. C.
and Kumar, Dinesh
and Jain, Shruti",
title="Multilayer Perceptron and Genetic Algorithm-Based Intrusion Detection Framework for Cloud Environment",
booktitle="Mobile Radio Communications and 5G Networks",
year="2021",
publisher="Springer Singapore",
address="Singapore",
pages="475--485",
abstract="The attractive characteristics of the cloud computing environment encourage its growth and penetration in various sections of society like government, education, entertainment, etc. The large-scale adoption of cloud computing not only provides services to users but also presents a wide attack landscape to the attackers and intruders in order to perform sophisticated attacks. Widespread implementation of cloud computing and its distributed and decentralized existence makes this computing paradigm prone to intrusion and attacks. Thus, the creation of network intrusion detection framework using anomaly detection method for cloud computing network with a better assault identification level and less false positives is essential. This paper discusses an effective network-based intrusion detection model utilizing artificial neural network strategies such as multilayer perceptron programmed with a genetic algorithm, as well as compares it using other machine learning techniques. The genetic algorithm was incorporated in multiple layer perceptron to predict the connection weights. Standard IDS dataset, namely CICIDS 2017, was used for simulation and testing of the suggested model. The results of implementation demonstrate the ability of the proposed model in the identification of intrusions in the cloud environment with a higher rate of detection and generation of minimal false alarm warnings, which suggests its dominance relative to state-of-the-art approaches. The implementation results show an accuracy of 90{\%}.",
note="Weights and biases",
isbn="978-981-15-7130-5"
}


@article{https://doi.org/10.1002/mmce.22542,
author = {Jarndal, Anwar and Husain, Saddam and Hashmi, Mohammad},
title = {Genetic algorithm initialized artificial neural network based temperature dependent small-signal modeling technique for GaN high electron mobility transistors},
journal = {International Journal of RF and Microwave Computer-Aided Engineering},
volume = {n/a},
number = {n/a},
pages = {e22542},
keywords = {ANN, cascade MLP, GaN-on-silicon HEMT, genetic algorithm, MLP, small-signal modeling},
doi = {https://doi.org/10.1002/mmce.22542},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mmce.22542},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mmce.22542},
abstract = {Abstract This paper explores and develops efficient temperature-dependent small-signal modeling approaches for GaN high electron mobility transistors (HEMTs). The multilayer perceptron (MLP) architecture and cascaded MLP architecture of artificial neural network are employed to model temperature dependence of 2-mm GaN-on-silicon device. It is identified that both architectures face problem of dependence on initials values of weights and biases. To overcome this issue, the genetic algorithm (GA) is incorporated in both MLP and cascaded MLP architectures. The models are trained on a large set of operating conditions (bias voltages and ambient temperatures) over a frequency range of 0.1 to 20 GHz and then tested for both temperature interpolation and extrapolation cases to assess their accuracy and robustness. An excellent agreement between the measured and the modeled S-parameters over the entire frequency range demonstrate the quality and robustness of the proposed technique. It is also shown that the cascaded MLP with GA exhibits better performance but with increased complexity.},
note={Evolves initial weights}
}


@INPROCEEDINGS{9308303,

  author={F. {Ishikawa} and L. Z. {Trovões} and L. {Carmo} and F. O. d. {França} and D. G. {Fantinato}},

  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 

  title={Playing Mega Man II with Neuroevolution}, 

  year={2020},

  volume={},

  number={},

  pages={2359-2364},
  note={Evolves only weights},
  doi={10.1109/SSCI47803.2020.9308303}}


@article{ALBADARNEH2020,
title = "Neuro-evolutionary models for imbalanced classification problems",
journal = "Journal of King Saud University - Computer and Information Sciences",
year = "2020",
issn = "1319-1578",
doi = "https://doi.org/10.1016/j.jksuci.2020.11.005",
url = "http://www.sciencedirect.com/science/article/pii/S1319157820305309",
author = "Israa Al-Badarneh and Maria Habib and Ibrahim Aljarah and Hossam Faris",
keywords = "Metaheuristics, Neural networks, Imbalanced classification",
note="Only weights and biases",
abstract = "Training an Artificial Neural Network (ANN) algorithm is not trivial, which requires optimizing a set of weights and biases that increase dramatically with the increasing capacity of the neural network resulting in such hard optimization problems. Essentially, over recent decades, stochastic search algorithms have shown remarkable abilities for addressing hard optimization problems. On the other hand, pragmatically, abundant real-world problems suffer from the imbalance problem, where the distribution of data varies considerably among classes resulting in more training biases and variances which degrades the performance of the learning algorithm. This paper introduces three stochastic and metaheuristic algorithms for training the Multilayer Perceptron (MLP) neural network to solve the problem of imbalanced classifications. The utilized algorithms are the Grey Wolf Optimization (GWO), Particle Swarm Optimization (PSO), and the Salp Swarm Algorithm (SSA). The proposed GWO-MLP, PSO-MLP, and SSA-MLP are trained based on different objective functions; accuracy, f1-score, and g-mean. Whereas, it is evaluated based on 10 benchmark imbalanced datasets. The results show an advantage for f1-score, and g-mean fitness functions over the accuracy when the datasets are imbalanced."
}

@article{parsian2017hybrid,
  title={A hybrid neural network-gray wolf optimization algorithm for melanoma detection},
  author={Parsian, Ali and Ramezani, Mehdi and Ghadimi, Noradin},
  year={2017},
  publisher={Biomedical Research},
  note={Only updated weights and biases}
}

@article{panda2019effective,
  title={How effective is spotted hyena optimizer for training multilayer perceptrons},
  author={Panda, Nibedan and Majhi, Santosh Kumar},
  journal={Int. J. Recent Technol. Eng},
  pages={4915--4927},
  year={2019},
  note={Only weights and biases}
}

@article{heidari2019efficient,
  title={An efficient hybrid multilayer perceptron neural network with grasshopper optimization},
  author={Heidari, Ali Asghar and Faris, Hossam and Aljarah, Ibrahim and Mirjalili, Seyedali},
  journal={Soft Computing},
  volume={23},
  number={17},
  pages={7941--7958},
  year={2019},
  publisher={Springer}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}


@inproceedings{zhu2010intelligent,
  title={Intelligent trading using support vector regression and multilayer perceptrons optimized with genetic algorithms},
  author={Zhu, Ming and Wang, Lipo},
  booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--5},
  year={2010},
  organization={IEEE}
}

@inproceedings{schiffmann1993application,
  title={Application of genetic algorithms to the construction of topologies for multilayer perceptrons},
  author={Schiffmann, Wolfram and Joost, Merten and Werner, Randolf},
  booktitle={Artificial Neural Nets and Genetic Algorithms},
  pages={675--682},
  year={1993},
  organization={Springer}
}

@misc{fua2020comparing,
      title={Comparing Python, Go, and C++ on the N-Queens Problem},
      author={Pascal Fua and Krzysztof Lis},
      year={2020},
      eprint={2001.02491},
      archivePrefix={arXiv},
      primaryClass={cs.MS}
}

@article{murtagh1991multilayer,
  title={Multilayer perceptrons for classification and regression},
  author={Murtagh, Fionn},
  journal={Neurocomputing},
  volume={2},
  number={5-6},
  pages={183--197},
  year={1991},
  publisher={Elsevier}
}

@INBOOK{6917150,
  author={E. {Alpaydin}},
  booktitle={Introduction to Machine Learning}, 

  title={Multilayer Perceptrons}, 

  year={2014},

  volume={},

  number={},

  pages={267-316},

  doi={}}

@article{alpaydin1994gal,
  title={GAL: Networks that grow when they learn and shrink when they forget},
  author={Alpaydin, Ethem},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  volume={8},
  number={01},
  pages={391--414},
  year={1994},
  publisher={World Scientific}
}

@article{yao1993evolutionary,
  title={Evolutionary artificial neural networks},
  author={Yao, Xin},
  journal={International journal of neural systems},
  volume={4},
  number={03},
  pages={203--222},
  year={1993},
  publisher={World Scientific}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Springer}
}

@article{mirjalili2015effective,
  title={How effective is the Grey Wolf optimizer in training multi-layer perceptrons},
  author={Mirjalili, Seyedali},
  journal={Applied Intelligence},
  volume={43},
  number={1},
  pages={150--161},
  year={2015},
  publisher={Springer},
  note={Only changes biases and weights}
}

@incollection{mirjalili2019evolutionary,
  title={Evolutionary multi-layer perceptron},
  author={Mirjalili, Seyedali},
  booktitle={Evolutionary Algorithms and Neural Networks},
  pages={87--104},
  year={2019},
  publisher={Springer}
}

@article{ecer2020training,
  title={Training Multilayer Perceptron with Genetic Algorithms and Particle Swarm Optimization for Modeling Stock Price Index Prediction},
  author={Ecer, Fatih and Ardabili, Sina and Band, Shahab S and Mosavi, Amir},
  journal={Entropy},
  volume={22},
  number={11},
  pages={1239},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{TAJMIRI2020108997,
title = "Evolving multilayer perceptron, and factorial design for modelling and optimization of dye decomposition by bio-synthetized nano CdS-diatomite composite",
journal = "Environmental Research",
volume = "182",
pages = "108997",
year = "2020",
issn = "0013-9351",
doi = "https://doi.org/10.1016/j.envres.2019.108997",
url = "http://www.sciencedirect.com/science/article/pii/S0013935119307947",
author = "Shadi Tajmiri and Ebrahim Azimi and Mohammad Raouf Hosseini and Yousef Azimi",
keywords = "Multilayer perceptron artificial neural network, Genetic algorithm, Factorial design, CdS nanoparticles, Bio-synthesize, Photocatalyst",
abstract = "Design of experiment and hybrid genetic algorithm optimized multilayer perceptron (GA-MLP) artificial neural network have been employed to model and predict dye decomposition capacity of the biologically synthesized nano CdS diatomite composite. Impact of independent variables such as, light (UV: on-off), solution pH (5–8), composite weight (CW: 0.5–1 mg), initial dye concentration (DC: 10–20 mg/l) and contact time (0–120 min), mainly in two levels, were examined to evaluate dye removal efficiency of the composite. According to the developed response surface based on the factorial design, all independent variables shown positive interactive effect on dye removal (UV > CW > pH > DC), as well as the pH-CW mutual interaction, while both UV-DC and CW-DC had antagonistic effect. The pH-CW interaction was more influential than pH and DC. Incorporation of the intermediate measurements of dye removal between the start and final contact times in GA-MLP approach, had found to improve the accuracy and predictability of the GA-MLP model. Based on the closeness of the R2 (0.98), root mean square error (1.03), variance accounted for (98.23\%), mean absolute error (0.61) and model predictive error (9.46\%) to their desirable levels, proposed GA-MLP model outperformed the factorial design model. Finally, optimal parameter choice for maximum dye removal using factorial design and GA-MLP were found as: UV (on), pH (9), CW (1 g) and DC (10 mg/l) and UV (on), pH (8.85), CW (0.92 g), DC (12.3 mg/l) and T (117 0.6 min), respectively."
}

@article{bengio2006greedy,
  title={Greedy layer-wise training of deep networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={153--160},
  year={2006}
}

@article{stanley2002evolving,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}

@book{nielsen2015neural,
  title={Neural networks and deep learning},
  author={Nielsen, Michael A},
  volume={2018},
  year={2015},
  publisher={Determination press San Francisco, CA}
}

@incollection{miikkulainen2019evolving,
  title={Evolving deep neural networks},
  author={Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and others},
  booktitle={Artificial Intelligence in the Age of Neural Networks and Brain Computing},
  pages={293--312},
  year={2019},
  publisher={Elsevier}
}
@incollection{bottou2012stochastic,
  title={Stochastic gradient descent tricks},
  author={Bottou, L{\'e}on},
  booktitle={Neural networks: Tricks of the trade},
  pages={421--436},
  year={2012},
  publisher={Springer}
}

@inproceedings{qolomany2017parameters,
  title={Parameters optimization of deep learning models using Particle swarm optimization},
  author={Qolomany, Basheer and Maabreh, Majdi and Al-Fuqaha, Ala and Gupta, Ajay and Benhaddou, Driss},
  booktitle={2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC)},
  pages={1285--1290},
  year={2017},
  organization={IEEE}
}

@book{goodfellow,
    place={Cambridge,MA},
    title={Deep learning},
    publisher={The MIT Press},
    author={Goodfellow,Ian and Bengio,Yoshua and Courville,Aaron},
    year={2016}
}

@misc{nielsen,
    author={Nielsen, Michael A.},
    title={Neural Networks and Deep Learning},
    howpublished={\url{http://neuralnetworksanddeeplearning.com/}},
    publisher={Determination Press},
    year={2015},
}

% 01_introduccion
@ARTICLE{evolutionary_computation,
    author={J. {Zhang} and Z. {Zhan} and Y. {Lin} and N. {Chen} and Y. {Gong} and
    J. {Zhong} and H. S. H. {Chung} and Y. {Li} and Y. {Shi}},
    journal={IEEE Computational Intelligence Magazine},
    title={Evolutionary Computation Meets Machine Learning: A Survey},
    year={2011},
    volume={6},
    number={4},
    pages={68-75},
    month={Nov},
}

@misc{g-prop,
    title = {G-Prop: Global optimization of multilayer perceptrons using GAs},
    journal = {Neurocomputing},
    volume = {35},
    number = {1},
    pages = {149 - 163},
    year = {2000},
    note = {Building information systems based on neural networks},
    issn = {0925-2312},
    howpublished = {\url{http://www.sciencedirect.com/science/article/pii/S0925231200003027}},
    author = {P.A. Castillo and J.J. Merelo and A. Prieto and V. Rivas and G. Romero},
    keywords = {Genetic algorithms, Neural networks, Optimization, Learning, Generalization},
}

@INPROCEEDINGS{tdd-use,
    author={L. {Williams} and E. M. {Maximilien} and M. {Vouk}},
    booktitle={14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.}, 
    title={Test-driven development as a defect-reduction practice}, 
    year={2003},
    volume={},
    number={},
    pages={34-45},
}

% 02_descripcion

@misc{ga-intro,
    author = {Vijini Mallawaarachchi},
    title = {Introduction to Genetic Algorithms},
    howpublished = {\url{https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3}}
}

@misc{nn-diagram,
    author = {Kjell Magne Fauske},
    title = {Example: Neural Network},
    howpublished = {\url{http://www.texample.net/tikz/examples/neural-network/}}
}

% 03_estado_del_arte

@misc{sklearn-mlpclassifier,
    author       = {scikit-learn developers},
    title        = {Scikit-learn: MLPClassifier},
    howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html}},
    year         = {2007-2019}
}

@article{sklearn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
            and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
            and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
            Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}

@misc{sklearn-grid,
    author       = {scikit-learn developers},
    title        = {Scikit-learn: Grid search},
    howpublished = {\url{https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search}},
    year         = {2007-2019}
}

@misc{sklearn-random,
    author       = {scikit-learn developers},
    title        = {Scikit-learn: Random search},
    howpublished = {\url{https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization}},
    year         = {2007-2019}
}

@misc{sklearn-deap,
    author       = {rsteca en GitHub},
    title        = {Use evolutionary algorithms instead of gridsearch in scikit-learn},
    howpublished = {\url{https://github.com/rsteca/sklearn-deap}},
    year         = {2015-2019}
}

@misc{keras-nn,
    title={Keras},
    author={Chollet, Fran\c{c}ois and others},
    year={2015},
    howpublished={\url{https://keras.io}},
}

@misc{keras-sequential,
    title={Keras: The Sequential Class},
    author={Chollet, Fran\c{c}ois and others},
    howpublished={\url{https://keras.io/api/models/sequential/}},
}

@misc{keras-dense,
    title={Keras: Dense layer},
    author={Chollet, Fran\c{c}ois and others},
    howpublished={\url{https://keras.io/api/layers/core_layers/dense//}},
}

@misc{tensorflow-nn,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
    year={2015},
}

@article{theano-nn,
    author = {{Theano Development Team}},
    title = "{Theano: A {Python} framework for fast computation of mathematical expressions}",
    journal = {arXiv e-prints},
    volume = {abs/1605.02688},
    primaryClass = "cs.SC",
    keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Computer Science - Mathematical Software},
    year = 2016,
    month = may,
    url = {http://arxiv.org/abs/1605.02688},
}

@incollection{pytorch-nn,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@MISC{torch-nn,
    author = {Ronan Collobert and Samy Bengio and Johnny Marithoz},
    title = {Torch: A Modular Machine Learning Software Library},
    year = {2002}
}

@article{sklearn-nn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
            and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
            and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
            Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}

@article{f-measure,
    author = {Hripcsak, George and Rothschild, Adam S.},
    title = "{Agreement, the F-Measure, and Reliability in Information Retrieval}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {12},
    number = {3},
    pages = {296-298},
    year = {2005},
    month = {05},
    abstract = "{Information retrieval studies that involve searching the Internet or marking phrases usually lack a well-defined number of negative cases. This prevents the use of traditional interrater reliability metrics like the κ statistic to assess the quality of expert-generated gold standards. Such studies often quantify system performance as precision, recall, and F-measure, or as agreement. It can be shown that the average F-measure among pairs of experts is numerically identical to the average positive specific agreement among experts and that κ approaches these measures as the number of negative cases grows large. Positive specific agreement—or the equivalent F-measure—may be an appropriate way to quantify interrater reliability and therefore to assess the reliability of a gold standard in these studies.}",
    issn = {1067-5027},
    doi = {10.1197/jamia.M1733},
    url = {https://doi.org/10.1197/jamia.M1733},
    eprint = {https://academic.oup.com/jamia/article-pdf/12/3/296/2429751/12-3-296.pdf},
}

@misc{mxnet-nn,
    author       = {The Apache Software Foundation (ASF)},
    title        = {MXNet},
    howpublished = {\url{https://mxnet.incubator.apache.org/}},
    year         = {2017-2018}
}

@article{deap-ga,
    author    = " F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e ",
    title     = { {DEAP}: Evolutionary Algorithms Made Easy },
    pages    = { 2171--2175 },
    volume    = { 13 },
    month     = { jul },
    year      = { 2012 },
    journal   = { Journal of Machine Learning Research }
}

@misc{deap-fitness,
    author = " F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e ",
    title = {{DEAP}: Fitness},
    howpublished = {\url{https://deap.readthedocs.io/en/master/api/base.html#fitness}},
}

@misc{platypus-ga,
    author       = {David Hadka},
    title        = {Platypus},
    howpublished = {\url{https://platypus.readthedocs.io/en/latest/index.html}},
    year         = {2015}
}

@online{tdd-definition,
    author       = {Agile Alliance},
    title        = {Test-driven development},
    howpublished = {\url{https://www.agilealliance.org/glossary/tdd/}},
}

@online{github,
    author       = {GitHub, Inc.},
    title        = {GitHub},
    howpublished = {\url{https://github.com/}},
    year         = {2007},
}

@online{kanban,
    author       = {Agile Alliance},
    title        = {Kanban board},
    howpublished = {\url{https://www.agilealliance.org/glossary/kanban-board/}},
}

% Analysis
@article{proben1,
    author = {Prechelt, Lutz and Informatik, Fakultat},
    year = {1995},
    month = {07},
    pages = {},
    title = {PROBEN1 - A Set of Neural Network Benchmark Problems and Benchmarking Rules}
}

@misc{uci,
    author = {Dua, Dheeru and Graff, Casey},
    year = {2017},
    title = {{UCI} Machine Learning Repository},
    howpublished = {\url{http://archive.ics.uci.edu/ml}},
    institution = {University of California, Irvine, School of Information and Computer Sciences},
} 

@article{dna-helicases,
    title={Six molecules of SV40 large T antigen assemble in a propeller-shaped particle around a channel},
    volume={268},
    DOI={10.1006/jmbi.1997.0952},
    number={1},
    journal={Journal of Molecular Biology},
    author={Martín M.carmen San and Gruss,Claudia and Carazo, José M.},
    year={1997},
    pages={15–20}
}

@online{py-click,
    author = {Pallets},
    title = {Click Python package},
    howpublished = {\url{https://click.palletsprojects.com/}},
    year = {2014},
}

@article{py-numpy,
  author={S. {van der Walt} and S. C. {Colbert} and G. {Varoquaux}},
  journal={Computing in Science   Engineering},
  title={The NumPy Array: A Structure for Efficient Numerical Computation},
  year={2011},
  volume={13},
  number={2},
  pages={22-30},
}

@misc{gplv3,
    author       = {Free Software Foundation},
    title        = {{GNU} {G}eneral {P}ublic {L}icense},
    version      = {3},
    howpublished = {\url{http://www.gnu.org/licenses/gpl.html}},
    shorthand    = {GPL},
}

@misc{deep-g-prop:anon,
    author       = {One A. Uthor},
    title        = {Document about Deep-G-Prop},
    howpublished = {BSc Thesis}
}

@software{deep-g-prop,
  author       = {Luis Liñán Villafranca and
                  Juan Julián Merelo Guervós},
  title        = {DeepGProp},
  month        = nov,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v1.1.0},
  doi          = {10.5281/zenodo.4287505},
  url          = {https://doi.org/10.5281/zenodo.4287505}
}

@misc{3blue1brown-mlp-1,
    author = {Grant Sanderson},
    title = {Neural networks},
    howpublished = {\url{https://www.youtube.com/watch?v=aircAruvnKk}},
}

@article{GUIJORUBIO2020118374,
title = "Evolutionary artificial neural networks for accurate solar radiation prediction",
journal = "Energy",
volume = "210",
pages = "118374",
year = "2020",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2020.118374",
url = "http://www.sciencedirect.com/science/article/pii/S036054422031481X",
author = "D. Guijo-Rubio and A.M. Durán-Rosal and P.A. Gutiérrez and A.M. Gómez-Orellana and C. Casanova-Mateo and J. Sanz-Justo and S. Salcedo-Sanz and C. Hervás-Martínez",
keywords = "Solar radiation estimation, Evolutionary artificial neural networks, Satellite data, Physical models",
abstract = "This paper evaluates the performance of different evolutionary neural network models in a problem of solar radiation prediction at Toledo, Spain. The prediction problem has been tackled exclusively from satellite-based measurements and variables, which avoids the use of data from ground stations or atmospheric soundings. Specifically, three types of neural computation approaches are considered: neural networks with sigmoid-based neurons, radial basis function units and product units. In all cases these neural computation algorithms are trained by means of evolutionary algorithms, leading to robust and accurate models for solar radiation prediction. The results obtained in the solar radiation estimation at the radiometric station of Toledo show an excellent performance of evolutionary neural networks tested. The structure sigmoid unit-product unit with evolutionary training has been shown as the best model among all tested in this paper, able to obtain an extremely accurate prediction of the solar radiation from satellite images data, and outperforming all other evolutionary neural networks tested, and alternative Machine Learning approaches such as Support Vector Regressors or Extreme Learning Machines."
}

@article{LUO20211,
title = "Using spotted hyena optimizer for training feedforward neural networks",
journal = "Cognitive Systems Research",
volume = "65",
pages = "1 - 16",
year = "2021",
issn = "1389-0417",
doi = "https://doi.org/10.1016/j.cogsys.2020.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S1389041720300577",
author = "Qifang Luo and Jie Li and Yongquan Zhou and Ling Liao",
keywords = "Spotted hyena optimizer, Feedforward neural networks, Classification datasets, Metaheuristic optimization",
abstract = "Spotted hyena optimizer (SHO) is a novel metaheuristic optimization algorithm based on the behavior of spotted hyena and their collaborative behavior in nature. In this paper, we design a spotted hyena optimizer for training feedforward neural network (FNN), which is regarded as a challenging task since it is easy to fall into local optima. Our objective is to apply metaheuristic optimization algorithm to tackle this problem better than the mathematical and deterministic methods. In order to confirm that using SHO to train FNN is more effective, five classification datasets and three function-approximations are applied to benchmark the performance of the proposed method. The experimental results show that the proposed SHO algorithm for optimization FNN has the best comprehensive performance and has more outstanding performance than other the state-of-the-art metaheuristic algorithms in terms of the performance measures."
}

@article{DBLP:journals/corr/abs-1712-06567,
  author    = {Felipe Petroski Such and
               Vashisht Madhavan and
               Edoardo Conti and
               Joel Lehman and
               Kenneth O. Stanley and
               Jeff Clune},
  title     = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative
               for Training Deep Neural Networks for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1712.06567},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.06567},
  archivePrefix = {arXiv},
  eprint    = {1712.06567},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-06567.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ding2013evolutionary,
  title={Evolutionary artificial neural networks: a review},
  author={Ding, Shifei and Li, Hui and Su, Chunyang and Yu, Junzhao and Jin, Fengxiang},
  journal={Artificial Intelligence Review},
  volume={39},
  number={3},
  pages={251--260},
  year={2013},
  publisher={Springer}
}

@inproceedings{jin2019auto,
  title={Auto-Keras: An Efficient Neural Architecture Search System},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1946--1956},
  year={2019},
  organization={ACM}
}

@article{brock2017smash,
  title={Smash: one-shot model architecture search through hypernetworks},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal={arXiv preprint arXiv:1708.05344},
  year={2017}
}
